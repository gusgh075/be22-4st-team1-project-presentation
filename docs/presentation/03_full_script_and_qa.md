# MapLog 발표 전체 대본 + 심사위원 압박 Q&A

> 총 발표 시간: 15분 / 구어체 기준 작성
> `[슬라이드 전환]` 표시 기준으로 슬라이드를 넘기세요.

---

## ▶ 1. 도입 — 서비스 기획 배경 및 문제 정의 (2분)

[슬라이드 전환 - 타이틀]

안녕하세요. MapLog 팀입니다.
저희 서비스를 한 문장으로 소개하자면,
**"나의 발자취를 지도 위에 기록하고, 친구와 공유하는 위치 기반 소셜 다이어리"** 입니다.

[슬라이드 전환 - Pain Point]

여러분은 여행을 다녀온 뒤 기억을 어떻게 남기시나요?
인스타그램에 사진을 올리거나, 메모 앱에 텍스트를 적는 분들이 많으실 텐데요.

저희가 주목한 Pain Point는 바로 여기에 있습니다.
기존 SNS나 다이어리 앱들은 **"언제"** 기준의 시간 순 나열에는 강하지만,
**"어디서"** 라는 공간적 맥락을 직관적으로 보여주지 못합니다.

내가 작년에 갔던 그 카페, 친구와 함께 방문한 그 골목,
기억 속의 장소를 다시 찾고 싶을 때 텍스트 기록만으로는 한계가 있었습니다.

[슬라이드 전환 - Solution]

그래서 저희는 지도를 서비스의 전면으로 가져왔습니다.
사용자는 지도 위에 핀을 꽂고 그 자리에 글과 사진을 기록합니다.
시간이 지나면 지도 위에 나만의 발자취가 쌓이고,
친구들과 피드를 통해 서로의 공간 기록을 공유할 수 있습니다.

단순히 어디를 갔는지 기억하는 것을 넘어,
**"그 장소에서의 나"** 를 오래도록 간직할 수 있는 서비스가 MapLog입니다.

---

## ▶ 2. 기술 스택 및 서비스 아키텍처 (3분)

[슬라이드 전환 - 아키텍처 다이어그램]

기술 스택을 말씀드리겠습니다.
크게 프론트엔드, 백엔드, 인프라 세 영역으로 나눠 설명드리겠습니다.

**프론트엔드**는 Vue 3의 Composition API와 Pinia를 사용했습니다.
인증 상태와 실시간 알림 상태를 Pinia Store에서 중앙 관리하고,
Vite를 통해 빠른 빌드와 환경 변수 분리를 구현했습니다.

**백엔드**는 Spring Boot 3.5와 Java 21을 기반으로 구성했습니다.
여기서 중요한 설계 결정이 있었는데요, CQRS 패턴을 도입했습니다.
각 도메인 패키지를 Command와 Query로 분리해서,
쓰기 작업은 JPA를 통해 강타입 엔티티 관리를,
읽기 작업은 더 유연한 쿼리 최적화를 적용할 수 있도록 구조를 설계했습니다.

[슬라이드 전환 - 인프라/CI-CD]

**인프라**는 저희가 가장 공을 들인 부분입니다.
컨테이너 환경은 Docker와 Kubernetes를 사용했고,
배포 파이프라인은 Jenkins와 ArgoCD를 결합한 GitOps 방식으로 구성했습니다.

개발자가 main 브랜치에 Push하면,
GitHub Webhook이 ngrok 터널을 통해 Jenkins를 트리거합니다.
Jenkins는 멀티스테이지 Dockerfile로 백엔드 JAR과 프론트엔드 정적 파일을 빌드하고,
이미지를 Docker Hub에 푸시한 뒤 매니페스트 레포지토리의 이미지 태그를 업데이트합니다.
ArgoCD는 이 매니페스트 변경을 감지해 Kubernetes 클러스터에 동기화합니다.

코드 Push 하나로 빌드부터 배포까지 이어지는 파이프라인을 직접 구축한 경험은
저희 팀에게 가장 값진 학습이었습니다.

---

## ▶ 3. 주요 기능 시연 (3분)

[슬라이드 전환 - 지도 화면]

이제 실제 서비스를 보여드리겠습니다.

메인 화면은 지도입니다.
사용자가 지도를 보다 기록하고 싶은 장소를 클릭하면 일기 작성 모달이 열립니다.
카카오 맵 SDK를 직접 제어해 지도 영역이 바뀔 때마다
화면 안에 있는 마커만 서버에서 가져오는 방식으로 성능을 최적화했습니다.

[슬라이드 전환 - 일기 작성 화면]

일기 작성 화면에서는 제목, 본문과 함께 사진을 첨부할 수 있습니다.
사진은 서버를 거쳐 AWS S3에 저장되고,
조회 시에는 1시간짜리 Presigned URL로 변환돼 전달됩니다.
이를 통해 S3 버킷을 외부에 직접 노출하지 않고 보안을 유지했습니다.

공개 범위를 설정할 수도 있어서, 나만 보는 일기와 친구에게 공개하는 일기를 구분할 수 있습니다.

[슬라이드 전환 - 소셜/알림 화면]

소셜 기능으로는 친구 추가와 피드 조회가 있습니다.
친구의 공개 일기가 피드 형식으로 보이고,
친구 요청이 오거나 수락되면 실시간 알림이 뜹니다.

실시간 알림은 **SSE, Server-Sent Events**로 구현했습니다.
로그인한 사용자는 서버와 지속적인 단방향 연결을 유지하고,
서버에서 이벤트가 발생하는 즉시 브라우저에 푸시됩니다.
WebSocket과 달리 HTTP 표준 위에서 동작하기 때문에
별도 프로토콜 업그레이드 없이 기존 보안 설정을 그대로 활용할 수 있었습니다.

---

## ▶ 4. 핵심 트러블슈팅 — 기술적 도전과 해결 (5분)

[슬라이드 전환 - 트러블슈팅 섹션]

이 프로젝트를 통해 저희가 가장 많이 성장한 순간들은 사실 이 섹션에 담겨 있습니다.
기능이 안 되는 순간, 왜 안 되는지 파고드는 과정에서 진짜 학습이 일어났습니다.
세 가지 사례를 공유하겠습니다.

---

### 사례 1 — 인증 시스템: Axios Interceptor 기반 자동 갱신

[슬라이드 전환 - 코드]

첫 번째는 인증 시스템입니다.

Access Token 만료 시간을 30분으로 설정했는데,
만료 순간 API 호출이 갑자기 실패해 사용자가 영문도 모른 채 오류를 마주하는 문제가 있었습니다.

해결 방법으로 Axios 인터셉터를 두 단계로 구성했습니다.
**요청 인터셉터**에서는 모든 API 호출에 Access Token을 자동으로 첨부합니다.
**응답 인터셉터**에서는 401이 오면 Refresh Token으로 새 토큰을 발급받고 원본 요청을 재시도합니다.

그런데 여기서 추가적인 문제가 생겼습니다.
한 화면에서 API 요청이 여러 개 동시에 나가다가 모두 401이 터지는 상황이 있었는데요,
각 요청이 독립적으로 토큰 갱신을 시도하면 Refresh Token이 중복 소비되는 Race Condition이 발생했습니다.

이를 `isRefreshing` 플래그와 `pendingQueue` 배열로 해결했습니다.
갱신이 시작되면 이후 들어오는 실패 요청들을 Queue에 쌓아두고,
갱신이 완료되는 순간 Queue에 있는 모든 요청을 한 번에 재시도하는 방식입니다.
덕분에 사용자는 토큰 만료를 전혀 인식하지 못한 채 서비스를 이용할 수 있게 됐습니다.

---

### 사례 2 — 환경별 파일 저장: @Profile 전략 패턴

[슬라이드 전환 - 코드]

두 번째는 파일 저장 전략입니다.

로컬 개발 중에는 로컬 파일시스템을 쓰고,
K8s 배포 후에는 AWS S3를 써야 했는데,
처음에는 환경 변수를 if 분기문으로 나누는 방식을 생각했습니다.

하지만 이 방식은 저장 로직 안에 인프라 판단이 뒤섞여 유지보수가 어려워집니다.

저희는 Spring의 `@Profile` 어노테이션을 활용한 전략 패턴을 선택했습니다.
`FileStorageService` 인터페이스를 정의하고,
`@Profile("local")`이 붙은 `LocalFileStorageService`,
`@Profile({"dev","aws"})`이 붙은 `S3FileStorageService`를 각각 구현했습니다.
서비스 레이어는 인터페이스만 주입받기 때문에
환경 변수 하나만 바꾸면 구현체가 자동으로 교체되고,
다이어리 저장 로직은 단 한 줄도 손댈 필요가 없습니다.

---

### 사례 3 — K8s 환경 이미지 401 권한 이슈

[슬라이드 전환 - 에러 흐름도]

세 번째가 가장 복잡했던 케이스입니다.

K8s 배포 후 일기 상세 페이지에서 이미지가 전혀 로드되지 않고,
브라우저 콘솔에 `GET /api/uploads/xxx.png 401 Unauthorized`가 찍혔습니다.

원인을 추적해보니 세 가지 레이어에 걸친 연쇄 문제였습니다.

**첫째**, K8s 환경에서 `SPRING_PROFILES_ACTIVE`가 지정되지 않아
S3가 아닌 `LocalFileStorageService`가 활성화됐고,
이미지가 Pod의 로컬 파일시스템에 저장되는 상황이었습니다.

**둘째**, 프론트엔드의 `toImageUrl` 함수가 상대경로 `/uploads/...`에
`VITE_API_BASE_URL=/api`를 붙여 `/api/uploads/...`를 만들어냈고,
`<img>` 태그는 Axios 인터셉터를 거치지 않아 인증 헤더가 전혀 붙지 않았습니다.

**셋째**, Spring Security 설정에서 `/uploads/**`만 `permitAll`로 열어두고
`/api/uploads/**` 경로는 인증 필요로 막혀 있었습니다.

결국 세 개의 레이어가 완벽하게 맞물려 401이 발생한 구조였습니다.

해결은 `@Profile` 매핑을 재설계하는 것이었습니다.
`S3FileStorageService`의 프로필을 `"aws"`에서 `"dev", "aws"` 둘 다로 확장하고,
`LocalFileStorageService`는 `"local"` 전용으로 명확히 분리했습니다.
이후 이미지는 S3에 저장되어 Presigned URL로 제공되므로
Nginx와 Spring Security를 완전히 우회해 브라우저가 S3에 직접 요청합니다.

이 경험을 통해 `spring.config.import`로 설정값을 로드하는 것과
`@Profile`로 Bean을 활성화하는 것이 완전히 별개의 레이어라는 것을 명확히 이해했습니다.

---

## ▶ 5. 마무리 — 성과 및 향후 발전 방향 (2분)

[슬라이드 전환 - 회고]

이번 프로젝트에서 저희가 얻은 가장 큰 수확 두 가지를 꼽겠습니다.

첫 번째는 **CI/CD 파이프라인을 직접 설계하고 운영한 경험**입니다.
Jenkins의 멀티스테이지 빌드 구성부터 ArgoCD의 GitOps 배포,
ngrok을 활용한 로컬 K8s 외부 노출까지 DevOps 전체 흐름을 손으로 만져봤습니다.
단순히 배포를 성공시키는 것을 넘어,
빌드 실패 시 Discord로 알림이 오고 빌드 번호로 이미지 버전을 추적하는 경험은
운영 환경이 어떻게 돌아가는지를 체감하게 해준 경험이었습니다.

두 번째는 **문제를 레이어별로 분리해서 분석하는 사고방식**입니다.
K8s 이미지 401 케이스처럼 하나의 에러 뒤에
프로필 설정, URL 조작, Security 설정이라는 세 개의 레이어가 얽혀 있을 때,
각 레이어를 독립적으로 검증하면서 원인을 좁혀가는 방법을 익혔습니다.

[슬라이드 전환 - 향후 계획]

향후에는 세 가지 방향으로 발전시키고 싶습니다.
방문 장소 기반 통계와 히트맵 시각화,
WebSocket을 이용한 친구 간 실시간 위치 공유,
그리고 PWA 지원을 통한 모바일 앱 경험 확장입니다.

[슬라이드 전환 - 마무리]

저희 MapLog는 단순한 기록 앱이 아닙니다.
사용자가 살아온 공간의 지도를 완성해가는 서비스입니다.

이상으로 발표를 마치겠습니다. 감사합니다.

---
---

# 심사위원 압박 Q&A — 5선

> 대상: 시니어 백엔드 / 데브옵스 엔지니어 심사위원
> 형식: 질문 → 핵심 포인트 → 모범 답변

---

## Q1. "현재 서비스 규모에서 CQRS 패턴이 실질적으로 필요했나요? 오버엔지니어링 아닌가요?"

**[왜 이 질문이 나오는가]**
소규모 서비스에 CQRS를 도입하면 코드량이 증가하고 복잡도만 높아진다는 비판이 있음.
심사위원은 기술을 "왜 썼는지" 근거를 검증하려는 것.

**[모범 답변]**

솔직히 말씀드리면, 현재 데이터 규모에서 CQRS가 성능 상의 필수 요건은 아니었습니다.
저희가 CQRS를 선택한 이유는 성능보다 **코드 복잡도 관리와 학습 목적**이 컸습니다.

다이어리 도메인만 보더라도, 쓰기 측은 JPA 엔티티의 연관관계와 비즈니스 검증이 중심이고,
읽기 측은 지도 마커 좌표 조회처럼 특정 컬럼만 가볍게 가져오는 쿼리가 반복됩니다.
이 두 요구사항을 하나의 Service/Repository에 섞으면 클래스가 빠르게 비대해지는 경험을 실제로 했습니다.

Command와 Query를 패키지 수준에서 분리하자 각 클래스의 역할이 명확해졌고,
팀원 간 코드 충돌도 줄었습니다.

말씀하신 것처럼 현재 규모에서 Full CQRS는 과도한 면이 있습니다.
만약 처음부터 다시 설계한다면, 도메인 수준 분리 없이 Service 레이어에서
ReadService와 WriteService를 분리하는 더 가벼운 방식도 고려했을 것입니다.
하지만 이 경험을 통해 읽기/쓰기 분리 설계의 이점을 직접 이해했다는 점에서 의미 있는 선택이었다고 생각합니다.

---

## Q2. "K8s 이미지 401 문제, 근본 원인이 Spring Security 설정 하나의 문제였나요? 실제로 원인이 몇 개였나요?"

**[왜 이 질문이 나오는가]**
대부분 지원자가 겉으로 드러난 하나의 원인만 말하고 끝냄.
실제로는 세 개의 레이어가 연쇄적으로 맞물린 복합 원인이었음을 파악했는지 검증하려는 것.

**[모범 답변]**

이 문제가 까다로웠던 이유는 원인이 하나가 아니라 **세 개의 독립적인 레이어**가 동시에 맞물린 복합 장애였기 때문입니다.

첫 번째 레이어는 **Spring Profile 활성화 오류**입니다.
K8s 배포 시 `SPRING_PROFILES_ACTIVE` 환경변수가 지정되지 않아
S3가 활성화되어야 할 환경에서 `LocalFileStorageService`가 동작했고,
이미지가 Pod의 ephemeral 파일시스템에 저장됐습니다.

두 번째 레이어는 **프론트엔드 URL 조합 로직 오류**입니다.
`toImageUrl` 함수가 `/uploads/uuid.png`라는 상대경로에
`VITE_API_BASE_URL=/api`를 붙여 `/api/uploads/uuid.png`를 만들었습니다.
그런데 `<img>` 태그는 Axios 인터셉터를 거치지 않아 Authorization 헤더가 전혀 없었습니다.

세 번째 레이어는 **Spring Security 허용 경로 불일치**입니다.
`/uploads/**`는 `permitAll`이었지만 `/api/uploads/**`는 인증 필요였습니다.
Nginx가 `/api/**`를 백엔드로 프록시하기 때문에, 인증 없는 요청이 인증 필요 경로로 들어온 것입니다.

세 가지가 동시에 맞물려야만 발생하는 문제였기 때문에,
하나씩 제거해가며 레이어를 분리해서 검증하는 방식으로 원인을 특정했습니다.

또 한 가지 중요한 발견이 있었는데, `spring.config.import`로 설정값을 로드하는 것과
`@Profile`로 Bean을 등록하는 것이 완전히 별개의 레이어라는 점입니다.
설정값이 로드됐다고 해서 그 프로필의 Bean이 활성화되는 것이 아님을
이 케이스를 통해 명확히 이해했습니다.

---

## Q3. "Refresh Token을 localStorage에 저장하셨는데, XSS 공격에 취약한 건 알고 계셨나요? 왜 HttpOnly Cookie를 선택하지 않으셨나요?"

**[왜 이 질문이 나오는가]**
보안에 대한 인식 수준을 검증하는 가장 날카로운 질문 중 하나.
localStorage는 JS로 직접 접근 가능하기 때문에 XSS 취약점이 있는 경우 토큰이 탈취됨.

**[모범 답변]**

네, 알고 있었습니다. localStorage가 XSS에 취약하다는 점은 저희도 인지하고 있었습니다.

HttpOnly Cookie는 JavaScript에서 접근이 불가능하기 때문에 XSS 공격 시 토큰 탈취를 막을 수 있습니다.
반면 저희가 선택한 localStorage는 JS로 직접 접근이 가능해, XSS 취약점이 존재하는 경우 토큰이 탈취될 수 있습니다.

저희가 localStorage를 선택한 실제 이유를 솔직하게 말씀드리면,
구현 복잡도를 낮추는 것이 주요 이유였습니다.
HttpOnly Cookie 방식을 선택했다면 서버의 `Set-Cookie` 응답 헤더 설정,
SameSite 속성 처리, CSRF 방지를 위한 토큰 추가 구현이 필요했는데,
프로젝트 기간 내에 모든 기능을 완성하는 것을 우선순위로 두다 보니 이 결정을 했습니다.

개선 방향을 말씀드리면, Refresh Token은 HttpOnly, Secure, SameSite=Strict Cookie로 저장하고,
Access Token은 메모리(Pinia store의 reactive 변수)에만 두는 방식이 더 안전합니다.
이 경우 XSS로 Refresh Token 탈취는 막고, CSRF 공격은 SameSite=Strict로 방어할 수 있습니다.
현재 구조의 보안 부채를 인식하고 있으며, 이것이 가장 먼저 개선해야 할 부분임을 알고 있습니다.

---

## Q4. "ArgoCD를 도입하셨는데 Sync Policy를 Manual로 설정한 이유가 뭔가요? GitOps의 핵심 이점인 자동 배포를 포기한 거 아닌가요?"

**[왜 이 질문이 나오는가]**
GitOps와 ArgoCD를 "왜 썼는지" 이해하고 있는지를 검증하는 질문.
도구를 도입했지만 그 도구의 핵심 철학을 역행하는 설정을 한 것에 대한 날카로운 지적.

**[모범 답변]**

날카로운 지적입니다. ArgoCD의 강점이 자동화인데 Manual Sync를 선택한 것은 확실히 그 이점을 반감시킵니다.

저희가 Manual을 선택한 이유는 현실적인 이유 때문이었습니다.
Docker Desktop 위의 단일 K8s 클러스터이고,
팀원 모두가 같은 클러스터를 공유하고 있어서
자동 Sync가 활성화되면 개발 중인 코드가 즉시 배포되어 다른 팀원의 작업을 덮어쓸 위험이 있었습니다.

또한 Automatic Sync 시 애플리케이션이 예상치 못한 상태로 변경되면
디버깅이 어려워질 수 있다는 판단도 있었습니다.

이상적인 운영 환경에서라면, 브랜치 전략을 함께 가져가야 한다고 생각합니다.
main 브랜치의 manifest를 production 클러스터에 Automatic Sync하고,
develop 브랜치의 manifest를 staging 클러스터에 Automatic Sync하는 방식이라면
자동 배포의 이점을 안전하게 누릴 수 있습니다.
저희 환경은 인프라가 단일 클러스터로 제한되어 이 구조를 갖추지 못한 것이 아쉬운 점입니다.

---

## Q5. "SSE로 실시간 알림을 구현하셨는데, 사용자가 많아지면 서버가 모든 사용자와 연결을 유지해야 합니다. 이 구조의 확장성 한계를 인지하고 있나요?"

**[왜 이 질문이 나오는가]**
SSE를 "단순히 구현했다"가 아니라 운영 환경에서의 한계를 이해하고 있는지 검증하는 질문.
특히 서버 인스턴스가 여러 개로 Scale-out 될 경우 SSE는 치명적인 문제가 생김.

**[모범 답변]**

맞습니다. SSE는 구조적으로 중요한 확장성 한계가 있습니다.

현재 저희 구조는 SseEmitter를 서버 메모리에 Map으로 보관합니다.
서버 인스턴스가 하나일 때는 문제가 없지만,
Scale-out으로 서버 인스턴스가 여러 개가 되면
A 서버에 연결된 사용자에게 B 서버가 이벤트를 보내야 할 때 emitter를 찾을 수 없습니다.
즉, **SSE는 기본적으로 Stateful 연결이기 때문에 수평 확장에 취약**합니다.

이를 해결하는 일반적인 방법은 두 가지입니다.
하나는 **Redis Pub/Sub**을 중간 브로커로 두는 방식입니다.
이벤트 발생 시 Redis 채널에 발행하면, 모든 서버 인스턴스가 구독해서 자신이 가진 emitter에 전달합니다.
다른 하나는 단순히 **Sticky Session**으로 같은 사용자의 요청이 항상 같은 인스턴스로 라우팅되게 하는 방식인데, 이는 진정한 의미의 Scale-out 해결책은 아닙니다.

현재 프로젝트는 단일 인스턴스 환경이라 이 문제가 드러나지 않았지만,
운영 환경에서는 Redis Pub/Sub 기반의 브로커 구조가 필수적이라고 인식하고 있습니다.
SSE 대신 WebSocket + STOMP + Redis를 활용한 메시지 브로커 구조를 다음 개선 목표로 두고 있습니다.
